{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCZBFzjClURz"
      },
      "source": [
        "# Assignment 4 - TinyML HelloWorld - Section 1\n",
        "Based on the **hello_world** example from [TensorFlow Lite for MicroControllers](https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/hello_world).\n",
        "\n",
        "© SkillSurf 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In this section you will train a Tensorflow model to a set of sinusoidal data. First you will synthesize the data to mimic a sine wave. Then you can build your own tensorflow model and fit the model to the generated data. Start by importing the relevant modules."
      ],
      "metadata": {
        "id": "Fqj9YNhvo828"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFnEB4CpoIKH"
      },
      "source": [
        "## Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIH9NN1c9PJn"
      },
      "outputs": [],
      "source": [
        "# Import Tensorflow and NumPy\n",
        "# Set random seed to get reproducible results\n",
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53PBJBv1jEtJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-PuBEb6CMeo"
      },
      "source": [
        "##  Create the Dataset\n",
        "\n",
        "You can use NumPy to generate a sinewave data and add some gaussian noise to make the data more realistic. The dataset will consist of 1000 datapoints (x-values) and relevant y-values. The following code creates a sine wave dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKjg7QeMDsDx"
      },
      "outputs": [],
      "source": [
        "# Number of sample datapoints\n",
        "SAMPLES = 1000\n",
        "\n",
        "# Generate a uniformly distributed set of random numbers in the range from\n",
        "# 0 to 2π, which covers a complete sine wave oscillation\n",
        "x_values = np.random.uniform(\n",
        "    low=0, high=2*math.pi, size=SAMPLES).astype(np.float32)\n",
        "\n",
        "# Shuffle the values to guarantee they're not in order\n",
        "np.random.shuffle(x_values)\n",
        "\n",
        "# Calculate the corresponding sine values\n",
        "y_values = np.sin(x_values).astype(np.float32)\n",
        "\n",
        "# Plot the data. The 'b.' argument tells the library to print blue dots.\n",
        "plt.plot(x_values, y_values, 'b.')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next add noise to the data to make the data more realistic. (In real-life the data we obtain usually get contaminated by different kinds of noise.)"
      ],
      "metadata": {
        "id": "SQ6r6j7kyKPZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0FJe3Y-Gkac"
      },
      "outputs": [],
      "source": [
        "# Add a small random number to each y value\n",
        "y_values += 0.1 * np.random.randn(*y_values.shape)\n",
        "\n",
        "# Plot our data\n",
        "plt.plot(x_values, y_values, 'b.')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up8Xk_pMH4Rt"
      },
      "source": [
        "## Pre-process data (Graded)\n",
        "\n",
        "The dataset has been given, now you will have to split this dataset into train, validation and test subsets. The following table shows the split ratio you should be using.\n",
        "\n",
        "| Split | Percentage |\n",
        "|----------|----------|\n",
        "| Train | 60% |\n",
        "| Validation | 20% |\n",
        "| Test | 20% |\n",
        "\n",
        "You may use the `np.split()` function for obtaining 3 splits of data from one line of code. You have to provide the indiced of points which the dataset is divided. The second argument to `np.split()` is an array of indices where the data will be split. We provide two indices, so the data will be divided into three chunks. For more clarification look into the documentation of [np.split()](https://numpy.org/doc/stable/reference/generated/numpy.split.html).\n",
        "\n",
        "\n",
        "### Exercise 1\n",
        "Complete the code below to split the data accordingly and plot all three splits in the same plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNYko5L1keqZ"
      },
      "outputs": [],
      "source": [
        "# Define the indices where the dataset will get chopped (TODO)\n",
        "TRAIN_SPLIT = ...\n",
        "TEST_SPLIT = ...\n",
        "\n",
        "# Use np.split to chop the data into three parts (TODO)\n",
        "x_train, x_test, x_validate = ...\n",
        "y_train, y_test, y_validate = ...\n",
        "\n",
        "# Double check that our splits add up correctly\n",
        "assert (x_train.size + x_validate.size + x_test.size) ==  SAMPLES\n",
        "\n",
        "# Plot the data in each partition in different colors\n",
        "plt.plot(x_train, y_train, 'b.', label=\"Train\")\n",
        "plt.plot(x_test, y_test, 'r.', label=\"Test\")\n",
        "plt.plot(x_validate, y_validate, 'g.', label=\"Validate\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQd0JSdOoAbw"
      },
      "source": [
        "## Build the Model (Graded)\n",
        "\n",
        "You have successfully pre-processed the dataset. Next you will have to define build the Tensorflow model using Keras. You may use the Tensorflow Keras Sequential API. Please refer to the official [Keras documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers) for further information. Use the below architecture to design your the model.\n",
        "\n",
        "- Input layer\n",
        "- 2 Dense layers each consisting of 16 hidden units and ReLU activation - [keras.layers.Dense()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)\n",
        "- Output layer with 1 unit\n",
        "\n",
        "### Exercise 2\n",
        "Design the sequential model according to the specifications above.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model using the Keras API (TODO)\n",
        "model = keras.Sequential([\n",
        "    ...\n",
        "])"
      ],
      "metadata": {
        "id": "TKnxOLyg2qEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have created the model, specify the optimizer, loss function and acuracy metrics. Use the below,\n",
        "\n",
        "- Optimizer: Adam\n",
        "- Loss function: Mean Squared Error (MSE)\n",
        "- Metric: Mean Absolute Error (MAE)\n",
        "\n",
        "You may use `model.compile()` and read the [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) documentation for this.\n",
        "\n",
        "### Exercise 3\n",
        "Set the optimizer and loss function details as specified as above."
      ],
      "metadata": {
        "id": "W_nrxVg25BkK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW0xus6AF-4o"
      },
      "outputs": [],
      "source": [
        "# Compile the model using a standard optimizer and loss function for regression\n",
        "model.compile(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeFBMPtsoIKL"
      },
      "outputs": [],
      "source": [
        "# Get model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv2SC409Grap"
      },
      "source": [
        "## Train the Model (Graded)\n",
        "\n",
        "Fit the model to the data using `model.fit()`. Train for 500 epochs with a batch size of 64. Use only the train and validation sets during training.\n",
        "\n",
        "### Exercise 4\n",
        "Fit the model to the data. Keep track of the losses and metrics using `history` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPAUrdkmGq1M"
      },
      "outputs": [],
      "source": [
        "# Fit the model to the data and keep track of losses (TODO)\n",
        "history = model.fit(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc_CQu2_IvOP"
      },
      "source": [
        "## Plotting Loss Curves\n",
        "\n",
        "The following code plots the loss curves (Training loss and validation loss) with each epoch. The loss curve can be used to check whether your model converged correctly.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-loss-curves-overfitting-underfitting-ideal.jpg\" alt=\"Loss Curves\" width=\"600\"/>\n",
        "\n",
        "Run the below cell and make sure your loss curves appear to be as the ones on the right of the below image. For more reading, refer to [this](https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYHGswAJJgrC"
      },
      "outputs": [],
      "source": [
        "# Draw a graph of the loss, which is the distance between\n",
        "# the predicted and actual values during training and validation.\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "# Exclude the first few epochs so the graph is easier to read\n",
        "SKIP = 100\n",
        "\n",
        "plt.plot(epochs[SKIP:], loss[SKIP:], 'g.', label='Training loss')\n",
        "plt.plot(epochs[SKIP:], val_loss[SKIP:], 'b.', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.clf()\n",
        "\n",
        "# Draw a graph of mean absolute error, which is another way of\n",
        "# measuring the amount of error in the prediction.\n",
        "mae = history.history['mae']\n",
        "val_mae = history.history['val_mae']\n",
        "\n",
        "plt.plot(epochs[SKIP:], mae[SKIP:], 'g.', label='Training MAE')\n",
        "plt.plot(epochs[SKIP:], val_mae[SKIP:], 'b.', label='Validation MAE')\n",
        "plt.title('Training and validation mean absolute error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f86dWOyZKmN9"
      },
      "source": [
        "## Predict using model (Graded)\n",
        "\n",
        "Use `model.predict()` to predict values for all data in test set and plot it against true values. You may refer to this [documentation](https://www.tensorflow.org/guide/keras/training_with_built_in_methods) for more information.\n",
        "\n",
        "### Exercise 5\n",
        "Predict y values for test data and plot it with true values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZfztKKyhLxX"
      },
      "outputs": [],
      "source": [
        "# Make predictions based on our test dataset (TODO)\n",
        "predictions = ...\n",
        "\n",
        "# Graph the predictions against the actual values\n",
        "plt.clf()\n",
        "plt.title('Comparison of predictions and actual values')\n",
        "plt.plot(x_test, y_test, 'b.', label='Actual')\n",
        "plt.plot(x_test, predictions, 'r.', label='Predicted')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 6\n",
        "The predicted graph is not nearly as smooth enough to be a sine. Rather it may look like a piecewise combination of linear functions. Briefly explain how you can make this more smoother and identical to an actual sine wave."
      ],
      "metadata": {
        "id": "quSdjqZ7BEbe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha-6Y4wZoIKM"
      },
      "source": [
        "## Get weights\n",
        "\n",
        "Now you will extract the weight matrices from the model. This step is in order to convert these weight matrices to C++ files that will be embedded in the Microcontroller."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVHwrZnOoIKM"
      },
      "outputs": [],
      "source": [
        "# Extract weights and biases as Numpy arrays\n",
        "W1, b1 = model.layers[0].get_weights()\n",
        "W2, b2 = model.layers[1].get_weights()\n",
        "W3, b3 = model.layers[2].get_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB7TirhPoIKM"
      },
      "outputs": [],
      "source": [
        "print(W1.shape, b1.shape)\n",
        "print(W2.shape, b2.shape)\n",
        "print(W3.shape, b3.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code performs the forward propagation of the model manually using NumPy matrix multiplication. Run the following code to make sure the model's prediction and manual prediction is same."
      ],
      "metadata": {
        "id": "NI6jaWlMAhAz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8Z2YPyGoIKM"
      },
      "outputs": [],
      "source": [
        "xs = x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWSAeSE9oIKM"
      },
      "outputs": [],
      "source": [
        "# This is what the TF model does internally:\n",
        "ys = []\n",
        "for x in xs:\n",
        "    x = np.array([x])       # x should be array\n",
        "    h1 = x @ W1 + b1        # dense layer\n",
        "    h1 = np.maximum(0, h1)  # ReLU\n",
        "    h2 = h1 @ W2 + b2       # dense layer\n",
        "    h2 = np.maximum(0, h2)  # ReLU\n",
        "    h3 = h2 @ W3 + b3       # dense layer\n",
        "    ys.append(h3)\n",
        "\n",
        "ys = np.stack(ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PV-X0qOoIKM"
      },
      "outputs": [],
      "source": [
        "# Are our predictions the same as TF's predictions?\n",
        "# This should print 0 if the results are close enough.\n",
        "np.sum(np.abs(ys - predictions) > 1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kheu2GIpoIKM"
      },
      "source": [
        "## Export the weights for C++\n",
        "\n",
        "The following code compresses the weights and biases into a C++ format which will be stored in the Microcontroller's FLASH memory. Copy the generated output and paste in file named `model_data.cpp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tgLV_dcoIKM"
      },
      "outputs": [],
      "source": [
        "# Note that we transpose W2. This makes the inner loop for the\n",
        "# matrix multiplication a little simpler.\n",
        "\n",
        "names = [\"W1_data\", \"b1_data\", \"W2_data\", \"b2_data\", \"W3_data\", \"b3_data\"]\n",
        "arrays = [W1, b1, W2.T, b2, W3, b3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOJKQWmIoIKN"
      },
      "outputs": [],
      "source": [
        "# Copy this into model_data.cpp:\n",
        "\n",
        "for name, array in zip(names, arrays):\n",
        "    print(\"const float %s[] PROGMEM = {\" % name)\n",
        "    print(\"   \", \", \".join([str(x) + \"f\" for x in array.flatten()]))\n",
        "    print(\"};\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End of Section 1"
      ],
      "metadata": {
        "id": "xFrLkSeiCejn"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}